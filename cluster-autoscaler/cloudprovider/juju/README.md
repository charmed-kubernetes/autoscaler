# Juju Cloud Provider

## Building
Currently, the Juju go API package breaks the azure cloud provider, since it brings in an incompatible version of the Azure SDK. Consequently, you must build ONLY the juju-provider autoscaler by executing the following command in the `cluster-autoscaler` directory:
```
make build BUILD_TAGS=juju
```

Note, the above command must be exected in the `cluster-autoscaler` directory, not the `juju` directory. 

## Running Unit Tests
Tests can be run by executing the following command inside the `juju` directory:
```
go test 
```
If you try and test the entire autoscaler application (i.e. run `go test` in the `cluster-autoscaler` directory) the tests will fail due to the Azure issues mentioned above. You must only run the Juju-related tests. 

Verbose output can be obtained using
```
go test -v
```

An HTML coverage report can be generated by executing the following command inside the `juju` directory:
```
go test -v -coverprofile cover.out .
go tool cover -html=cover.out -o cover.html
```

If the `JujuClient` interface defined in `juju_manager.go` changes, then new mocks will need to be generated to facilitate testing. To do this, execute the following command inside the `juju` directory:
```
go generate ./...
```
## Testing the Application Locally
After building the application, you can run and test it locally provided you have a Juju model with kubernetes available. 

You can use a simple image like nginx to load the Kubernetes worker nodes. For example, before starting the autoscaler, create the following YAML file:

```yaml
# nginx.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  replicas: 5
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
```

and apply it using kubectl:
```
kubectl apply -f nginx.yaml
```

You can check the pods are running using:
```
kubectl get pods
```

You should only see 5 of them. 

Next we need to create a configuration file to pass to the autoscaler so that Juju authentication is possible. Create the following cloud-config.yaml file:
```yaml
# cloud-config.yaml
user: user # Replace with your user
password: password # Replace with your user
endpoints:
  - 192.0.2.10:12345 # Replace with your endpoint(s)
ca-cert: |
  -----BEGIN CERTIFICATE-----
  <YOUR CERT HERE>
  -----END CERTIFICATE-----
```

You can obtain the values needed using the commands below:
```
KUBE_CONTROLLER=<your-kubernetes-controller>
API_ENDPOINTS=$(juju show-controller $KUBE_CONTROLLER --format json | jq -rc '.[].details["api-endpoints"] | join(",")' )
CA_CERT=$(juju show-controller $KUBE_CONTROLLER --format json | jq -rc '.[].details["ca-cert"]')
USERNAME=$(juju show-controller $KUBE_CONTROLLER --format json | jq -rc '.[].account.user')
PASSWORD=$(juju show-controller $KUBE_CONTROLLER --show-password --format json | jq -rc '.[].account.password')
```

You will also need to know the UUID of the model that contains the application you are wanting to scale (usually this application is kubernetes-worker). You can get this using:

```
juju models -c $KUBE_CONTROLLER --format json | jq -cr '.models[]|{name,"model-uuid"}'
```

Then you can execute the autoscaler application using the following command:
```
./cluster-autoscaler-amd64 -v=5 --kubeconfig=/absolute/path/to/your/.kube/config --cloud-provider=juju --max-node-provision-time 60m0s --scale-down-unneeded-time=5m0s --scale-down-delay-after-add 5m0s --nodes 3:5:<your-model-UUID>:kubernetes-worker --cloud-config=/absolute/path/to/your/cloud-config.yaml
```

Note that you will need to pass in absolute paths to your kube-config file as well as your cloud-config file. The `--nodes` option is used to specify the minimum and maximum number of nodes allowed, along with the model UUID and application name that you want to scale (kubernetes-worker in this case). Make sure you replace `<your-model-UUID>` with the UUID of the model found in the above step. 

Once that is running, wait until the main loop of the autoscaler starts. Assuming you had 3 worker nodes before starting the autoscaler, you should see output similar to the following in the log:
```
Skipping juju-3e6bc5-88 - node group min size reached
Skipping juju-3e6bc5-73 - node group min size reached
Skipping juju-3e6bc5-74 - node group min size reached
```

You can now increase the load on the worker nodes by scaling the deployment you made earlier. In a new terminal, run
```
kubectl scale --replicas=500 deployments/nginx
```

You will see many events in the log, and shortly after if you run `juju status` you should notice 2 more Juju machines being created, and 2 more worker units added in your `juju status` output. This brings you to 5 worker nodes (the maximum specified using the --nodes option)

It may take some time, but the 2 new workers should eventually reach active status. Once that happens, you can try scaling down by decreasing the load on the worker nodes back down to only 5 pods:
```
kubectl scale --replicas=5 deployments/nginx
```

The autoscaler should automatically remove unnecssary nodes. It may take some time, but you can check `juju status` output to verify that 2 of the worker nodes have been removed and you are back to only 3 worker nodes (the minimum specified using the --nodes option). 

## Executing a Rocks release:

The makefile in the cluster-autoscaler directory contains rules for creating a release of the application. This release process will build a multiarch image manifest along with images for that manifest. It will then push that manifest and images into rocks. To execute such a release, change to the cluster-autoscaler directory and run the following command:
```
make TAG=your-desired-image-tag  BUILD_TAGS=juju release
```

You will be prompted for confirmation several times before the images are pushed into rocks. After the process is complete, you can view the new tags [here](https://rocks.canonical.com/v2/cdk/cluster-autoscaler-juju/tags/list)
